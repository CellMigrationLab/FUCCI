{"cells":[{"cell_type":"markdown","source":["# **Spots analysis plotting**\n","\n","This notebook plots:\n","\n","*   peak distances in conditions\n","*   peak fwhm in conditions\n","* heatmaps of tracks\n","* Quality control of the spots data (peaks)\n","\n","Written by Joanna Pylvänäinen\n","\n","joanna.pylvanainen@abo.fi"],"metadata":{"id":"U61344k20N1k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzebO-DQ7fp_","cellView":"form"},"outputs":[],"source":["# @title #Mount GDrive\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install fastdtw\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"00-bB41W1jzx"},"outputs":[],"source":["# @title #Load useful functions\n","\n","\n","def normalize_to_01(series):\n","    \"\"\"\n","    Normalize a given time series (1D numpy array) between 0 and 1.\n","\n","    Parameters:\n","        series (numpy.ndarray): The time series to be normalized.\n","\n","    Returns:\n","        numpy.ndarray: The normalized time series.\n","    \"\"\"\n","    Imin = np.min(series)\n","    Imax = np.max(series)\n","\n","    if Imax == Imin:\n","        return np.zeros_like(series)\n","\n","    return (series - Imin) / (Imax - Imin)\n","\n","# Function to normalize by maximum amplitude\n","def normalize_by_max_amplitude(series):\n","    return series / np.max(np.abs(series))"]},{"cell_type":"markdown","metadata":{"id":"j1q7L5au1NV7"},"source":["\n","\n","---\n","\n","\n","# **Part 2: Analysing spot data**\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","source":["# @title #Define results folder (raw data). csv files will be fetched from here.\n","\n","# Set the Results_Folder path\n","Results_Folder = \"/content/drive/MyDrive/Kurppa/2025_reanalysis/Cleaned_notebooks/results_spots\"  # @param {type: \"string\"}\n","\n","print(f\"Results will be saved in: {Results_Folder}\")\n"],"metadata":{"cellView":"form","id":"D3M8-Cn-ODKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Load average_fwhm data and plot (raw data)\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","import numpy as np\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","\n","# Construct the file path within the Results_Folder\n","file_path = os.path.join(Results_Folder, 'data_for_Average_fwhm.csv')\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv(file_path)\n","\n","# Initialize PDF\n","pdf_path = os.path.join(Results_Folder, 'average_fwhm_raw.pdf')\n","print(pdf_path)\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a new figure and axes\n","fig, ax = plt.subplots(figsize=(10, 5))  # Create a figure and an axes object.\n","\n","# Sort the 'Condition' column alphabetically\n","group_order = df['Condition'].sort_values().unique()\n","\n","# Create the boxplot for pooled data on the specified axes\n","sns.boxplot(x='Condition', y='Average_fwhm', data=df,\n","            color='lightgray', order=group_order, ax=ax)  # Specify the axes to plot on\n","\n","# Overlay with a stripplot showing individual repeats on the specified axes\n","sns.stripplot(x='Condition', y='Average_fwhm', data=df,\n","              hue='Repeat', dodge=True, jitter=True,\n","              palette='magma', alpha=0.5, order=group_order, ax=ax)  # Specify the axes to plot on\n","\n","# Add a title to the plot\n","plt.title('Average Full Width at Half Maximum (FWHM) by Condition and Repeat (raw)')\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)  # Save the figure to the PDF\n","pdf_pages.close()\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"0GVz8NuWenQ6","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for Average_fwhm (raw data)\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_Average_fwhm.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the Average_fwhm column\n","df = df.dropna(subset=['Average_fwhm'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['Average_fwhm'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_Average_fwhm_raw.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n"],"metadata":{"id":"SZsDDw2keteD","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Load average peak distance data, add Group information and plot (raw data)\n","\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","import numpy as np\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","\n","# Load the CSV file into a pandas DataFrame\n","# Construct the file path within the Results_Folder\n","file_path = os.path.join(Results_Folder, 'data_for_Average_Peak_Distance.csv')\n","df = pd.read_csv(file_path)\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'average_peak_distance_raw.pdf')\n","print(pdf_path)\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a new figure and axes\n","fig, ax = plt.subplots(figsize=(10, 5)) #Create a figure and an axes object.\n","\n","# Sort the 'Condition' column alphabetically\n","group_order = df['Condition'].sort_values().unique()\n","\n","# Create the boxplot for pooled data on the specified axes\n","sns.boxplot(x='Condition', y='Average_Peak_Distance', data=df,\n","            color='lightgray', order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Overlay with a stripplot showing individual repeats on the specified axes\n","sns.stripplot(x='Condition', y='Average_Peak_Distance', data=df,\n","              hue='Repeat', dodge=True, jitter=True,\n","              palette='magma', alpha=0.5, order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Add a title to the plot\n","plt.title('Average Peak Distance by Condition and Repeat (raw)')\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)  # Save the figure to the PDF\n","pdf_pages.close()\n","\n","\n","\n","\n","\n"],"metadata":{"id":"svID58BUe-Zy","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for average_peak_distance (raw data)\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_Average_Peak_Distance.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the Average_fwhm column\n","df = df.dropna(subset=['Average_Peak_Distance'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['Average_Peak_Distance'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_Average_Peak_Distance_raw.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n","\n"],"metadata":{"id":"QoMLACUafHMM","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","# **Part 3: Data Balancing**\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"kGaojJucfMZA"}},{"cell_type":"code","source":["# @title ##Check the number of track per condition per repeats for plotting fwhm\n","\n","import os\n","import matplotlib.pyplot as plt\n","import pandas as pd # Import pandas\n","\n","data_for_fwhm = os.path.join(Results_Folder, 'data_for_Average_fwhm.csv')\n","data_for_df= pd.read_csv(data_for_fwhm) # Read the CSV file into a DataFrame\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram_fwhm.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","\n","# Make sure the QC folder exists\n","qc_folder = os.path.join(Results_Folder, \"QC\")\n","if not os.path.exists(qc_folder):\n","    os.makedirs(qc_folder)\n","\n","result_df = count_tracks_by_condition_and_repeat(data_for_df, qc_folder) # Pass the DataFrame 'data_for_df' to the function\n","\n","\n","\n"],"metadata":{"id":"cSFeoaQbEFyU","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ##Run this cell to downsample and balance your fwhm dataset\n","\n","!pip install tqdm  # Install the tqdm module\n","from tqdm import tqdm  # Import the tqdm function\n","import pandas as pd\n","import numpy as np\n","import os  # Import os to handle file paths\n","\n","def balance_dataset_by_condition_and_repeat(df, condition_col='Condition', repeat_col='Repeat', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling rows for each (Condition, Repeat) group\n","    to match the smallest group size.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced row counts across (Condition, Repeat) groups.\n","    \"\"\"\n","    np.random.seed(random_seed)  # Ensure reproducibility\n","\n","    # Count the number of rows per (Condition, Repeat) combination\n","    group_counts = df.groupby([condition_col, repeat_col]).size()\n","    min_row_count = group_counts.min()  # Find the smallest group size\n","\n","    print(f\"Balancing to {min_row_count} rows per (Condition, Repeat)\")\n","\n","    # Function to sample rows\n","    def sample_rows(group):\n","        return group.sample(n=min_row_count, replace=False, random_state=random_seed)\n","\n","    # Apply sampling to ensure equal row counts per (Condition, Repeat)\n","    balanced_df = df.groupby([condition_col, repeat_col], group_keys=False).apply(sample_rows)\n","\n","    return balanced_df.reset_index(drop=True)\n","\n","def replace_inf_with_nan(df, df_name):\n","    \"\"\"\n","    Replaces all infinite values (positive or negative infinity) in the DataFrame with NaN\n","    and prints a message for each column where infinities are found.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to replace inf values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","\n","    Returns:\n","    pd.DataFrame: DataFrame with infinity values replaced by NaN.\n","    \"\"\"\n","    inf_columns = df.columns[(df == np.inf).any() | (df == -np.inf).any()].tolist()\n","\n","    if inf_columns:\n","        for col in inf_columns:\n","            inf_count = ((df[col] == np.inf) | (df[col] == -np.inf)).sum()\n","            print(f\"Column '{col}' in {df_name} contains {inf_count} infinity values. Replacing with NaN.\")\n","\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    return df  # Return the modified DataFrame\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","    It first converts infinite values to NaNs before the check.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    df = replace_inf_with_nan(df, df_name)\n","\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar without compression.\"\"\"\n","\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        with open(path, \"w\") as f:\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","random_seed = 42\n","\n","if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n","\n","# Check how many tracks exist per condition and repeat BEFORE balancing\n","print(data_for_df.groupby(['Condition', 'Repeat'])['Unique_ID'].nunique())\n","\n","# Run the balancing function\n","balanced_data_for_df = balance_dataset_by_condition_and_repeat(data_for_df, random_seed=42)\n","\n","# Check how many rows exist per (Condition, Repeat) AFTER balancing\n","print(balanced_data_for_df.groupby(['Condition', 'Repeat']).size())\n","\n","result_df = count_tracks_by_condition_and_repeat(balanced_data_for_df, f\"{Results_Folder}/Balanced_dataset\")\n","\n","check_for_nans(balanced_data_for_df, \"balanced_data_for_df\")\n","save_dataframe_with_progress(balanced_data_for_df, Results_Folder + '/Balanced_dataset/fwhm_balanced_dataset.csv')\n"],"metadata":{"id":"C5uVUgDPNCt5","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Load average_fwhm data and plot (balanced data)\n","\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","import numpy as np\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'Balanced_dataset/fwhm_balanced_dataset.csv')\n","df = pd.read_csv(file_path)\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'average_fwhm_balanced.pdf')\n","print(pdf_path)\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a new figure and axes\n","fig, ax = plt.subplots(figsize=(10, 5)) #Create a figure and an axes object.\n","\n","# Sort the 'Condition' column alphabetically\n","group_order = df['Condition'].sort_values().unique()\n","\n","# Create the boxplot for pooled data on the specified axes\n","sns.boxplot(x='Condition', y='Average_fwhm', data=df,\n","            color='lightgray', order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Overlay with a stripplot showing individual repeats on the specified axes\n","sns.stripplot(x='Condition', y='Average_fwhm', data=df,\n","              hue='Repeat', dodge=True, jitter=True,\n","              palette='magma', alpha=0.5, order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Add a title to the plot\n","plt.title('Average Full Width at Half Maximum (FWHM) by Condition and Repeat')\n","\n","# Set the y-axis limits # This is the added line\n","ax.set_ylim(0, 60)\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)  # Save the figure to the PDF\n","pdf_pages.close()\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"8Ht_fk8-fqmy","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for Average_fwhm (balanced data)\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'Balanced_dataset/fwhm_balanced_dataset.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the Average_fwhm column\n","df = df.dropna(subset=['Average_fwhm'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['Average_fwhm'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_Average_fwhm_balanced.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n"],"metadata":{"id":"J5mt60uQ8PRe","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ##Check the number of track per condition per repeats for plotting deak distances\n","\n","import os\n","import matplotlib.pyplot as plt\n","import pandas as pd # Import pandas\n","\n","data_for_peaks = os.path.join(Results_Folder, 'data_for_Average_Peak_Distance.csv')\n","peaks_df= pd.read_csv(data_for_peaks) # Read the CSV file into a DataFrame\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram_peaks.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","\n","# Make sure the QC folder exists\n","qc_folder = os.path.join(Results_Folder, \"QC\")\n","if not os.path.exists(qc_folder):\n","    os.makedirs(qc_folder)\n","\n","result_df = count_tracks_by_condition_and_repeat(peaks_df, qc_folder) # Pass the DataFrame 'data_for_df' to the function\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"WyROuHHFsNzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ##Run this cell to downsample and balance your peaks dataset\n","\n","!pip install tqdm  # Install the tqdm module\n","from tqdm import tqdm  # Import the tqdm function\n","import pandas as pd\n","import numpy as np\n","import os  # Import os to handle file paths\n","\n","def balance_dataset_by_condition_and_repeat(df, condition_col='Condition', repeat_col='Repeat', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling rows for each (Condition, Repeat) group\n","    to match the smallest group size.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced row counts across (Condition, Repeat) groups.\n","    \"\"\"\n","    np.random.seed(random_seed)  # Ensure reproducibility\n","\n","    # Count the number of rows per (Condition, Repeat) combination\n","    group_counts = df.groupby([condition_col, repeat_col]).size()\n","    min_row_count = group_counts.min()  # Find the smallest group size\n","\n","    print(f\"Balancing to {min_row_count} rows per (Condition, Repeat)\")\n","\n","    # Function to sample rows\n","    def sample_rows(group):\n","        return group.sample(n=min_row_count, replace=False, random_state=random_seed)\n","\n","    # Apply sampling to ensure equal row counts per (Condition, Repeat)\n","    balanced_df = df.groupby([condition_col, repeat_col], group_keys=False).apply(sample_rows)\n","\n","    return balanced_df.reset_index(drop=True)\n","\n","def replace_inf_with_nan(df, df_name):\n","    \"\"\"\n","    Replaces all infinite values (positive or negative infinity) in the DataFrame with NaN\n","    and prints a message for each column where infinities are found.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to replace inf values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","\n","    Returns:\n","    pd.DataFrame: DataFrame with infinity values replaced by NaN.\n","    \"\"\"\n","    inf_columns = df.columns[(df == np.inf).any() | (df == -np.inf).any()].tolist()\n","\n","    if inf_columns:\n","        for col in inf_columns:\n","            inf_count = ((df[col] == np.inf) | (df[col] == -np.inf)).sum()\n","            print(f\"Column '{col}' in {df_name} contains {inf_count} infinity values. Replacing with NaN.\")\n","\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    return df  # Return the modified DataFrame\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","    It first converts infinite values to NaNs before the check.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    df = replace_inf_with_nan(df, df_name)\n","\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar without compression.\"\"\"\n","\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        with open(path, \"w\") as f:\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","random_seed = 42\n","\n","if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n","\n","# Check how many tracks exist per condition and repeat BEFORE balancing\n","print(peaks_df.groupby(['Condition', 'Repeat'])['Unique_ID'].nunique())\n","\n","# Run the balancing function\n","balanced_peaks_df = balance_dataset_by_condition_and_repeat(peaks_df, random_seed=42)\n","\n","# Check how many rows exist per (Condition, Repeat) AFTER balancing\n","print(balanced_peaks_df.groupby(['Condition', 'Repeat']).size())\n","\n","result_df = count_tracks_by_condition_and_repeat(balanced_peaks_df, f\"{Results_Folder}/Balanced_dataset\")\n","\n","check_for_nans(balanced_peaks_df, \"balanced_peaks_df\")\n","save_dataframe_with_progress(balanced_peaks_df, Results_Folder + '/Balanced_dataset/peaks_balanced_dataset.csv')\n"],"metadata":{"cellView":"form","id":"lA_BntJHsy6o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Load peak_distances data and plot (balanced data)\n","\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","import numpy as np\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","\n","# Load the CSV file into a pandas DataFrame\n","\n","file_path = os.path.join(Results_Folder, 'Balanced_dataset/peaks_balanced_dataset.csv')\n","df = pd.read_csv(file_path)\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'average_peaks_balanced.pdf')\n","print(pdf_path)\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a new figure and axes\n","fig, ax = plt.subplots(figsize=(10, 5)) #Create a figure and an axes object.\n","\n","# Sort the 'Condition' column alphabetically\n","group_order = df['Condition'].sort_values().unique()\n","\n","# Create the boxplot for pooled data on the specified axes\n","sns.boxplot(x='Condition', y='Average_Peak_Distance', data=df,\n","            color='lightgray', order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Overlay with a stripplot showing individual repeats on the specified axes\n","sns.stripplot(x='Condition', y='Average_Peak_Distance', data=df,\n","              hue='Repeat', dodge=True, jitter=True,\n","              palette='magma', alpha=0.5, order=group_order, ax=ax) #Specify the axes to plot on\n","\n","# Add a title to the plot\n","plt.title('Average Peak distance by Condition and Repeat')\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)  # Save the figure to the PDF\n","pdf_pages.close()\n"],"metadata":{"id":"UUWitqED-At0","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for average_peak_distance balanced\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'Balanced_dataset/peaks_balanced_dataset.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the Average_fwhm column\n","df = df.dropna(subset=['Average_Peak_Distance'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['Average_Peak_Distance'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_Average_Peak_Distance_balanced.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')"],"metadata":{"id":"HI6yemKlC-92","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Load average peak distance data, add Group information and plot\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","import numpy as np\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'Balanced_dataset/peaks_balanced_dataset.csv')\n","\n","# User input for distance cutoff\n","distance_cutoff_frames = 6  # @param {type: \"integer\"}\n","\n","df = pd.read_csv(file_path)\n","\n","# Filter out data points below the distance cutoff\n","df_filt = df[df['Average_Peak_Distance'] >= distance_cutoff_frames]\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, f'average_peak_distance_balanced_(Distance >= {distance_cutoff_frames}).pdf')  # Updated file name\n","print(pdf_path)\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a new figure and axes\n","fig, ax = plt.subplots(figsize=(10, 5))\n","\n","# Sort the 'Condition' column alphabetically\n","group_order = df_filt['Condition'].sort_values().unique()\n","\n","# Create the boxplot for pooled data\n","sns.boxplot(x='Condition', y='Average_Peak_Distance', data=df_filt,\n","            color='lightgray', order=group_order, ax=ax)\n","\n","# Overlay with a stripplot showing individual repeats\n","sns.stripplot(x='Condition', y='Average_Peak_Distance', data=df_filt,\n","              hue='Repeat', dodge=True, jitter=True,\n","              palette='magma', alpha=0.5, order=group_order, ax=ax)\n","\n","# Add a title to the plot\n","plt.title(f'Average Peak Distance by Condition and Repeat (Distance >= {distance_cutoff_frames})')  # Updated title\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)\n","pdf_pages.close()"],"metadata":{"cellView":"form","id":"8-nUU57tVK1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for average_peak_distance with cut-off from previous cell\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Drop rows with NaN values in the Average_fwhm column\n","df = df_filt.dropna(subset=['Average_Peak_Distance'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['Average_Peak_Distance'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","\n","results_csv_path = os.path.join(Results_Folder, (f't_test_Average_Peak_Distance_balanced (Distance >= {distance_cutoff_frames}).csv'))\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')"],"metadata":{"cellView":"form","id":"jjekoEPRWNpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot heatmaps (balanced, normalized, red)\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","# User inputs\n","Folder_path = os.path.join(Results_Folder, 'merged_Spots_with_NumPeaks.csv')\n","\n","# Load data\n","merged_df_balanced = pd.read_csv(Folder_path)\n","\n","# Google Colab Form Inputs\n","min_NUMBER_SPOTS = False  # @param {type:\"boolean\"}\n","min_TRACK_DURATION = True  # @param {type:\"boolean\"}\n","min_value = 90  # @param {type:\"number\"}  # Enter the minimum value for the selected variable\n","num_tracks_to_select = 20  # @param {type: \"integer\"}\n","\n","# Define filter condition based on the selected variable\n","if min_NUMBER_SPOTS and not min_TRACK_DURATION:\n","    filtered_df = merged_df_balanced[merged_df_balanced['NUMBER_SPOTS'] >= min_value]\n","elif min_TRACK_DURATION and not min_NUMBER_SPOTS:\n","    filtered_df = merged_df_balanced[merged_df_balanced['TRACK_DURATION'] >= min_value]\n","else:\n","    print(\"Select only one variable to filter using the minimum value.\")\n","    filtered_df = merged_df_balanced.copy()\n","\n","# Ensure 'Condition' column is present\n","if 'Condition' not in filtered_df.columns:\n","    filtered_df = pd.merge(filtered_df, merged_df_balanced[['Unique_ID', 'Condition', 'Num_peaks']], on='Unique_ID', how='left')\n","    print(\"Warning: 'Condition' column was missing and has been added back.\")\n","\n","# Save filtered dataframe for inspection\n","Results_Folder = os.path.dirname(Folder_path)\n","filtered_df.to_csv(os.path.join(Results_Folder, 'filtered_df.csv'), index=False)\n","\n","# Sort tracks by TRACK_DURATION in descending order\n","#filtered_df = filtered_df.sort_values(by='TRACK_DURATION', ascending=False)\n","#filtered_df = filtered_df.sort_values(by=['TRACK_DURATION', 'Num_Peaks'], ascending=False) #Filtering is done here\n","\n","\n","# Correctly select num_tracks_to_select unique tracks per condition using Unique_ID\n","selected_tracks = (\n","    filtered_df.groupby('Condition')\n","    .apply(lambda x: x['Unique_ID'].drop_duplicates().head(min(num_tracks_to_select, len(x))))\n","    .reset_index(drop=True)\n",")\n","\n","# Filter the original dataframe to include only selected tracks based on Unique_ID\n","filtered_tracks_df = filtered_df[filtered_df['Unique_ID'].isin(selected_tracks)]\n","\n","# Create a combined column for \"Condition_Unique_ID\" (using Unique_ID instead of TRACK_ID)\n","filtered_tracks_df['Condition_Unique_ID'] = (\n","    filtered_tracks_df['Condition'].astype(str) + '_' + filtered_tracks_df['Unique_ID'].astype(str)\n",")\n","\n","# Create a pivot table with the new column (using Condition_Unique_ID)\n","heatmap_data = (\n","    filtered_tracks_df\n","    .pivot_table(index='POSITION_T', columns='Condition_Unique_ID', values='MEAN_INTENSITY_CH2', aggfunc='mean')\n",")\n","\n","# Normalize each column between 0 and 1 (before filling NaN values)\n","def normalize(series):\n","    return (series - series.min()) / (series.max() - series.min())\n","\n","\n","heatmap_data = heatmap_data.apply(normalize, axis=0)\n","\n","# Create a mask for NaN values (True where NaN, False elsewhere)\n","nan_mask = heatmap_data.isna()\n","\n","# Plotting the heatmap with masked NaN values in light gray\n","plt.figure(figsize=(14, 10))\n","sns.set(style=\"white\")  # Set background to white\n","\n","\n","# Create a custom colormap with light gray for NaN values\n","cmap = plt.cm.viridis  # Your original colormap\n","cmap.set_bad('black')  # Set color for bad (NaN) values also lightgray\n","\n","# Use mask to hide NaN regions and show them in light gray\n","ax = sns.heatmap(\n","    heatmap_data,\n","    mask=nan_mask,\n","    cmap=cmap,  # Use the modified colormap\n","    cbar_kws={'label': 'Normalized Mean Intensity (CH2)'},\n","    linewidths=0.0,\n","    linecolor='white'\n",")\n","\n","# Adjust x-axis labels to display conditions only\n","conditions, track_ids = zip(*[col.split('_', 1) for col in heatmap_data.columns])\n","heatmap_data.columns = conditions\n","unique_conditions, condition_positions = np.unique(conditions, return_index=True)\n","centered_positions = [pos + (num_tracks_to_select / 2) - 0.5 for pos in condition_positions]\n","ax.set_xticks(centered_positions)\n","ax.set_xticklabels(unique_conditions, rotation=0, ha='center', fontsize=10, fontweight='bold')\n","\n","plt.xlabel('Condition')\n","plt.ylabel('Time (POSITION_T)')\n","plt.title('Normalized Heatmap of MEAN_INTENSITY_CH2 Over Time by Condition')\n","\n","plt.savefig(os.path.join(Results_Folder, 'heatmap_red_normalized.pdf'), bbox_inches='tight') # This line saves the heatmap\n","\n","plt.show()\n","\n","plt.show()\n","\n","# Add a line to print the number of tracks plotted per condition\n","for condition in unique_conditions:\n","    num_tracks = len(filtered_tracks_df[filtered_tracks_df['Condition'] == condition]['Unique_ID'].unique())\n","    print(f\"Number of tracks plotted for {condition}: {num_tracks}\")\n","\n","plt.show()\n","\n","# Print TRACK_DURATION and Num_Peaks for each plotted track\n","for condition in unique_conditions:\n","    tracks_in_condition = filtered_tracks_df[filtered_tracks_df['Condition'] == condition]\n","    unique_track_ids = tracks_in_condition['Unique_ID'].unique()\n","\n","    print(f\"\\nTracks plotted for {condition}:\")\n","    for track_id in unique_track_ids:\n","        track_duration = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['TRACK_DURATION'].iloc[0]\n","        num_peaks = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['Num_Peaks'].iloc[0]  # Get Num_Peaks\n","        print(f\"  Track ID: {track_id}, TRACK_DURATION: {track_duration}, Num_Peaks: {num_peaks}\")\n","\n","plt.show()\n"],"metadata":{"id":"GjohvGI6foSb","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot heatmaps (balanced, not normalized, red)\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","# User inputs\n","Folder_path = os.path.join(Results_Folder, 'merged_Spots_with_NumPeaks.csv')\n","\n","# Load data\n","merged_df_balanced = pd.read_csv(Folder_path)\n","\n","# Google Colab Form Inputs\n","min_NUMBER_SPOTS = False  # @param {type:\"boolean\"}\n","min_TRACK_DURATION = True  # @param {type:\"boolean\"}\n","min_value = 80  # @param {type:\"number\"}  # Enter the minimum value for the selected variable\n","num_tracks_to_select = 20  # @param {type: \"integer\"}\n","\n","# Define filter condition based on the selected variable\n","if min_NUMBER_SPOTS and not min_TRACK_DURATION:\n","    filtered_df = merged_df_balanced[merged_df_balanced['NUMBER_SPOTS'] >= min_value]\n","elif min_TRACK_DURATION and not min_NUMBER_SPOTS:\n","    filtered_df = merged_df_balanced[merged_df_balanced['TRACK_DURATION'] >= min_value]\n","else:\n","    print(\"Select only one variable to filter using the minimum value.\")\n","    filtered_df = merged_df_balanced.copy()\n","\n","# Ensure 'Condition' column is present\n","if 'Condition' not in filtered_df.columns:\n","    filtered_df = pd.merge(filtered_df, merged_df_balanced[['Unique_ID', 'Condition', 'Num_peaks']], on='Unique_ID', how='left')\n","    print(\"Warning: 'Condition' column was missing and has been added back.\")\n","\n","# Save filtered dataframe for inspection\n","Results_Folder = os.path.dirname(Folder_path)\n","filtered_df.to_csv(os.path.join(Results_Folder, 'filtered_df.csv'), index=False)\n","\n","# Sort tracks by TRACK_DURATION in descending order\n","filtered_df = filtered_df.sort_values(by=['TRACK_DURATION', 'Num_Peaks'], ascending=False)\n","\n","# Correctly select num_tracks_to_select unique tracks per condition using Unique_ID\n","selected_tracks = (\n","    filtered_df.groupby('Condition')\n","    .apply(lambda x: x['Unique_ID'].drop_duplicates().head(min(num_tracks_to_select, len(x))))\n","    .reset_index(drop=True)\n",")\n","\n","# Filter the original dataframe to include only selected tracks based on Unique_ID\n","filtered_tracks_df = filtered_df[filtered_df['Unique_ID'].isin(selected_tracks)]\n","\n","# Create a combined column for \"Condition_Unique_ID\" (using Unique_ID instead of TRACK_ID)\n","filtered_tracks_df['Condition_Unique_ID'] = (\n","    filtered_tracks_df['Condition'].astype(str) + '_' + filtered_tracks_df['Unique_ID'].astype(str)\n",")\n","\n","# Create a pivot table with the new column (using Condition_Unique_ID)\n","heatmap_data = (\n","    filtered_tracks_df\n","    .pivot_table(index='POSITION_T', columns='Condition_Unique_ID', values='MEAN_INTENSITY_CH2', aggfunc='mean')\n",")\n","\n","# Create a mask for NaN values (True where NaN, False elsewhere)\n","nan_mask = heatmap_data.isna()\n","\n","# Plotting the heatmap with masked NaN values in light gray\n","plt.figure(figsize=(14, 10))\n","sns.set(style=\"white\")  # Set background to white\n","\n","# Create a custom colormap with light gray for NaN values\n","cmap = plt.cm.viridis  # Your original colormap\n","cmap.set_bad('black')  # Set color for bad (NaN) values also lightgray\n","\n","# Use mask to hide NaN regions and show them in light gray\n","ax = sns.heatmap(\n","    heatmap_data,\n","    mask=nan_mask,\n","    cmap=cmap,  # Use the modified colormap\n","    cbar_kws={'label': 'Mean Intensity (CH2)'},  # Removed 'Normalized' from label\n","    linewidths=0.0,\n","    linecolor='white'\n",")\n","\n","# Adjust x-axis labels to display conditions only\n","conditions, track_ids = zip(*[col.split('_', 1) for col in heatmap_data.columns])\n","heatmap_data.columns = conditions\n","unique_conditions, condition_positions = np.unique(conditions, return_index=True)\n","centered_positions = [pos + (num_tracks_to_select / 2) - 0.5 for pos in condition_positions]\n","ax.set_xticks(centered_positions)\n","ax.set_xticklabels(unique_conditions, rotation=0, ha='center', fontsize=10, fontweight='bold')\n","\n","plt.xlabel('Condition')\n","plt.ylabel('Time (POSITION_T)')\n","plt.title('Heatmap of MEAN_INTENSITY_CH2 Over Time by Condition')  # Updated title\n","plt.savefig(os.path.join(Results_Folder, 'heatmap_red_raw.pdf'), bbox_inches='tight') # This line saves the heatmap\n","\n","plt.show()\n","\n","# Add a line to print the number of tracks plotted per condition\n","for condition in unique_conditions:\n","    num_tracks = len(filtered_tracks_df[filtered_tracks_df['Condition'] == condition]['Unique_ID'].unique())\n","    print(f\"Number of tracks plotted for {condition}: {num_tracks}\")\n","\n","plt.show()\n","\n","# Print TRACK_DURATION and Num_Peaks for each plotted track\n","for condition in unique_conditions:\n","    tracks_in_condition = filtered_tracks_df[filtered_tracks_df['Condition'] == condition]\n","    unique_track_ids = tracks_in_condition['Unique_ID'].unique()\n","\n","    print(f\"\\nTracks plotted for {condition}:\")\n","    for track_id in unique_track_ids:\n","        track_duration = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['TRACK_DURATION'].iloc[0]\n","        num_peaks = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['Num_Peaks'].iloc[0]  # Get Num_Peaks\n","        print(f\"  Track ID: {track_id}, TRACK_DURATION: {track_duration}, Num_Peaks: {num_peaks}\")\n","\n","plt.show()\n"],"metadata":{"cellView":"form","id":"obJeLl-8WgD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot heatmaps (balanced, normalized, green)\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","# User inputs\n","Folder_path = os.path.join(Results_Folder, 'merged_Spots_with_NumPeaks.csv')\n","\n","# Load data\n","merged_df_balanced = pd.read_csv(Folder_path)\n","\n","# Google Colab Form Inputs\n","min_NUMBER_SPOTS = False  # @param {type:\"boolean\"}\n","min_TRACK_DURATION = True  # @param {type:\"boolean\"}\n","min_value = 70  # @param {type:\"number\"}  # Enter the minimum value for the selected variable\n","num_tracks_to_select = 20  # @param {type: \"integer\"}\n","\n","# Define filter condition based on the selected variable\n","if min_NUMBER_SPOTS and not min_TRACK_DURATION:\n","    filtered_df = merged_df_balanced[merged_df_balanced['NUMBER_SPOTS'] >= min_value]\n","elif min_TRACK_DURATION and not min_NUMBER_SPOTS:\n","    filtered_df = merged_df_balanced[merged_df_balanced['TRACK_DURATION'] >= min_value]\n","else:\n","    print(\"Select only one variable to filter using the minimum value.\")\n","    filtered_df = merged_df_balanced.copy()\n","\n","# Ensure 'Condition' column is present\n","if 'Condition' not in filtered_df.columns:\n","    filtered_df = pd.merge(filtered_df, merged_df_balanced[['Unique_ID', 'Condition', 'Num_peaks']], on='Unique_ID', how='left')\n","    print(\"Warning: 'Condition' column was missing and has been added back.\")\n","\n","# Save filtered dataframe for inspection\n","Results_Folder = os.path.dirname(Folder_path)\n","filtered_df.to_csv(os.path.join(Results_Folder, 'filtered_df.csv'), index=False)\n","\n","# Sort tracks by TRACK_DURATION in descending order\n","#filtered_df = filtered_df.sort_values(by='TRACK_DURATION', ascending=False)\n","filtered_df = filtered_df.sort_values(by=['TRACK_DURATION', 'Num_Peaks'], ascending=False)\n","\n","\n","# Correctly select num_tracks_to_select unique tracks per condition using Unique_ID\n","selected_tracks = (\n","    filtered_df.groupby('Condition')\n","    .apply(lambda x: x['Unique_ID'].drop_duplicates().head(min(num_tracks_to_select, len(x))))\n","    .reset_index(drop=True)\n",")\n","\n","# Filter the original dataframe to include only selected tracks based on Unique_ID\n","filtered_tracks_df = filtered_df[filtered_df['Unique_ID'].isin(selected_tracks)]\n","\n","# Create a combined column for \"Condition_Unique_ID\" (using Unique_ID instead of TRACK_ID)\n","filtered_tracks_df['Condition_Unique_ID'] = (\n","    filtered_tracks_df['Condition'].astype(str) + '_' + filtered_tracks_df['Unique_ID'].astype(str)\n",")\n","\n","# Create a pivot table with the new column (using Condition_Unique_ID)\n","heatmap_data = (\n","    filtered_tracks_df\n","    .pivot_table(index='POSITION_T', columns='Condition_Unique_ID', values='MEAN_INTENSITY_CH1', aggfunc='mean')\n",")\n","\n","# Normalize each column between 0 and 1 (before filling NaN values)\n","def normalize(series):\n","    return (series - series.min()) / (series.max() - series.min())\n","\n","\n","heatmap_data = heatmap_data.apply(normalize, axis=0)\n","\n","# Create a mask for NaN values (True where NaN, False elsewhere)\n","nan_mask = heatmap_data.isna()\n","\n","# Plotting the heatmap with masked NaN values in light gray\n","plt.figure(figsize=(14, 10))\n","sns.set(style=\"white\")  # Set background to white\n","\n","\n","# Create a custom colormap with light gray for NaN values\n","cmap = plt.cm.viridis  # Your original colormap\n","cmap.set_bad('black')  # Set color for bad (NaN) values also lightgray\n","\n","# Use mask to hide NaN regions and show them in light gray\n","ax = sns.heatmap(\n","    heatmap_data,\n","    mask=nan_mask,\n","    cmap=cmap,  # Use the modified colormap\n","    cbar_kws={'label': 'Normalized Mean Intensity (CH1)'},\n","    linewidths=0.0,\n","    linecolor='white'\n",")\n","\n","# Adjust x-axis labels to display conditions only\n","conditions, track_ids = zip(*[col.split('_', 1) for col in heatmap_data.columns])\n","heatmap_data.columns = conditions\n","unique_conditions, condition_positions = np.unique(conditions, return_index=True)\n","centered_positions = [pos + (num_tracks_to_select / 2) - 0.5 for pos in condition_positions]\n","ax.set_xticks(centered_positions)\n","ax.set_xticklabels(unique_conditions, rotation=0, ha='center', fontsize=10, fontweight='bold')\n","\n","plt.xlabel('Condition')\n","plt.ylabel('Time (POSITION_T)')\n","plt.title('Normalized Heatmap of MEAN_INTENSITY_CH1 Over Time by Condition')\n","plt.savefig(os.path.join(Results_Folder, 'heatmap_green_normalized.pdf'), bbox_inches='tight') # This line saves the heatmap\n","\n","plt.show()\n","\n","# Add a line to print the number of tracks plotted per condition\n","for condition in unique_conditions:\n","    num_tracks = len(filtered_tracks_df[filtered_tracks_df['Condition'] == condition]['Unique_ID'].unique())\n","    print(f\"Number of tracks plotted for {condition}: {num_tracks}\")\n","\n","plt.show()\n","\n","# Print TRACK_DURATION and Num_Peaks for each plotted track\n","for condition in unique_conditions:\n","    tracks_in_condition = filtered_tracks_df[filtered_tracks_df['Condition'] == condition]\n","    unique_track_ids = tracks_in_condition['Unique_ID'].unique()\n","\n","    print(f\"\\nTracks plotted for {condition}:\")\n","    for track_id in unique_track_ids:\n","        track_duration = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['TRACK_DURATION'].iloc[0]\n","        num_peaks = tracks_in_condition[tracks_in_condition['Unique_ID'] == track_id]['Num_Peaks'].iloc[0]  # Get Num_Peaks\n","        print(f\"  Track ID: {track_id}, TRACK_DURATION: {track_duration}, Num_Peaks: {num_peaks}\")\n","\n","plt.show()\n"],"metadata":{"id":"_Ey8eBHMjY5u","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot % of tracks with peaks\n","\n","# Import necessary libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np  # Added import for np\n","\n","# Load the dataset (Ensure the file exists in the correct path)\n","file_path = os.path.join(Results_Folder, 'peak_info_all.csv')\n","peak_info = pd.read_csv(file_path)\n","\n","# --- Ensure 'Has_Peak' is properly mapped ---\n","# Convert to string, handle case sensitivity, and map to boolean\n","peak_info['Has_Peak'] = peak_info['Has_Peak'].astype(str).str.upper().map({'TRUE': True, 'FALSE': False})\n","\n","# Check for invalid values in 'Has_Peak'\n","if peak_info['Has_Peak'].isnull().any():\n","    print(\"Warning: Some 'Has_Peak' values could not be converted. Check the data.\")\n","\n","# Convert boolean to numeric (1 for True, 0 for False)\n","peak_info['Has_Peak'] = peak_info['Has_Peak'].fillna(False).astype(int)\n","\n","# --- Compute the percentage of peaks grouped by Condition and Repeat ---\n","peak_percentages = peak_info.groupby(['Condition', 'Repeat'])['Has_Peak'].mean() * 100\n","\n","# Convert to a DataFrame for plotting\n","peak_percentages = peak_percentages.reset_index()\n","\n","# Check if there is data to plot\n","if peak_percentages.empty:\n","    print(\"No data to plot. Check if 'Condition' and 'Repeat' columns are correct.\")\n","else:\n","    # --- Bar Chart ---\n","    plt.figure(figsize=(12, 6))\n","    sns.barplot(x='Condition', y='Has_Peak', hue='Repeat', data=peak_percentages)\n","    plt.ylabel('% of Tracks with Peaks')\n","    plt.xlabel('Condition')\n","    plt.title('Percentage of Tracks with Peaks by Condition and Repeat')\n","    plt.legend(title='Repeat', bbox_to_anchor=(1, 1))\n","    plt.xticks(rotation=45)\n","    plt.savefig(os.path.join(Results_Folder, 'QC/procentage_of_tracks_per_condition.pdf'), bbox_inches='tight') # This line saves the heatmap\n","    plt.show()\n","\n","    # --- Heatmap ---\n","    plt.figure(figsize=(10, 6))\n","    pivot_table = peak_percentages.pivot(index=\"Repeat\", columns=\"Condition\", values=\"Has_Peak\").fillna(0)\n","    sns.heatmap(pivot_table, annot=True, cmap=\"coolwarm\", fmt=\".1f\", linewidths=0.5)\n","    plt.xlabel('Condition')\n","    plt.ylabel('Repeat')\n","    plt.title('Heatmap of % Tracks with Peaks by Condition and Repeat')\n","    plt.savefig(os.path.join(Results_Folder, 'QC/procentage_of_tracks_per_condition_heatmap.pdf'), bbox_inches='tight') # This line saves the heatmap\n","    plt.show()\n"],"metadata":{"id":"r-jwBJZsoVYb","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot histogram of the track peak count in each condition\n","\n","# Import necessary libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Load the dataset (Ensure the file exists in the correct path)\n","file_path = os.path.join(Results_Folder, 'merged_Spots_with_NumPeaks.csv')\n","merged_df_balanced = pd.read_csv(file_path)\n","\n","# Ensure Num_Peaks is numeric\n","merged_df_balanced['Num_Peaks'] = pd.to_numeric(merged_df_balanced['Num_Peaks'], errors='coerce')\n","\n","# Drop NaN values in Num_Peaks\n","merged_df_balanced = merged_df_balanced.dropna(subset=['Num_Peaks'])\n","\n","# Aggregate by Unique_ID: Taking the AVERAGE Num_Peaks for each Unique_ID within each Condition\n","agg_df = merged_df_balanced.groupby(['Unique_ID', 'Condition'], as_index=False)['Num_Peaks'].mean()\n","\n","# Get the max value for setting x-ticks, ensuring it's an integer\n","max_value = int(agg_df['Num_Peaks'].max())\n","\n","# Create the figure\n","plt.figure(figsize=(12, 6))\n","\n","# Plot line for each Condition\n","for condition in agg_df['Condition'].unique():\n","    subset = agg_df[agg_df['Condition'] == condition]\n","\n","    # Compute count of each Num_Peaks value\n","    peak_counts = subset['Num_Peaks'].value_counts().sort_index()\n","\n","    # Normalize to get density\n","    peak_density = peak_counts / peak_counts.sum()\n","\n","    # Plot as a line\n","    plt.plot(peak_density.index, peak_density.values, marker='o', linestyle='-', label=condition)\n","\n","# Set x-ticks to show all integer values\n","plt.xticks(range(0, max_value + 1, 1))\n","\n","# Labels and title\n","plt.xlabel('Average Number of Peaks per Unique_ID')\n","plt.ylabel('Density')\n","plt.title('Number of Peaks Distribution by Condition (Averaged per Unique_ID)')\n","plt.legend(title='Condition')\n","plt.savefig(os.path.join(Results_Folder, 'QC/no_of_peaks_histogram.pdf'), bbox_inches='tight') # This line saves the heatmap\n","\n","# Show the plot\n","plt.show()\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"Z14ibzAY52r6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}