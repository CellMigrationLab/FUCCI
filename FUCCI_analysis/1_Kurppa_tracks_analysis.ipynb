{"cells":[{"cell_type":"markdown","source":["# **Track analysis for csv files from TrackMate**\n","\n","\n","This notebook measures:\n","\n","*   Mean tracking speed\n","*   Division time mean\n","\n","Written by Joanna Pylvänäinen\n","\n","joanna.pylvanainen@abo.fi"],"metadata":{"id":"hdjFXTkV5gMB"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FzebO-DQ7fp_"},"outputs":[],"source":["# @title #Mount GDrive\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install fastdtw\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"00-bB41W1jzx"},"outputs":[],"source":["# @title #Load useful functions\n","\n","\n","def normalize_to_01(series):\n","    \"\"\"\n","    Normalize a given time series (1D numpy array) between 0 and 1.\n","\n","    Parameters:\n","        series (numpy.ndarray): The time series to be normalized.\n","\n","    Returns:\n","        numpy.ndarray: The normalized time series.\n","    \"\"\"\n","    Imin = np.min(series)\n","    Imax = np.max(series)\n","\n","    if Imax == Imin:\n","        return np.zeros_like(series)\n","\n","    return (series - Imin) / (Imax - Imin)\n","\n","# Function to normalize by maximum amplitude\n","def normalize_by_max_amplitude(series):\n","    return series / np.max(np.abs(series))"]},{"cell_type":"markdown","metadata":{"id":"AaLlpzvt7w64"},"source":["# **Part 1: Analysing track data**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BP4zRE8_6Pfh","cellView":"form"},"outputs":[],"source":["# @title #Load tracks\n","\n","\n","import pandas as pd\n","import glob\n","import os\n","import numpy as np\n","\n","Folder_path = ''  # @param {type: \"string\"}\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","\n","# Initialize an empty list to collect DataFrames\n","df_list = []\n","\n","# Use glob to match the filename pattern\n","for filepath in glob.glob(Folder_path+'/*tracks*.csv'):\n","    # Extract well and FOV information from the filename\n","    filename = os.path.basename(filepath)\n","    file_name_without_ext = os.path.splitext(filename)[0].replace('_tracking-tracks', '')\n","    well_info = filename[0:3]\n","    fov_info = filename.split('_')[1]\n","\n","    # Read each CSV file into a DataFrame\n","    df = pd.read_csv(filepath, skiprows=[1, 2, 3])\n","\n","    # Add well, FOV, and file name information as new columns to the DataFrame\n","    df['Well'] = well_info\n","    df['FOV'] = fov_info\n","    df['File Name'] = file_name_without_ext\n","\n","    # Add a new column 'Condition' based on the 'Well' values\n","    df['Condition'] = np.select(\n","        [df['Well'].isin(['C02', 'C03', 'C04', 'C05', 'C06']),\n","         df['Well'].isin(['C07', 'C08', 'C09', 'C10', 'C11']),\n","         df['Well'].isin(['F02', 'F03', 'F04', 'F05', 'F06']),\n","         df['Well'].isin(['F07', 'F08', 'F09', 'F10', 'F11'])],\n","        ['Control pool', 'Control single cell', 'Mutation #34', 'Mutation #38'],\n","        default='Unknown'\n","    )\n","\n","    # Create a new column 'Repeat' to label repeats within each condition based on wells\n","    well_order = {'C02': 1, 'C03': 2, 'C04': 3, 'C05': 4, 'C06': 5,\n","                  'C07': 1, 'C08': 2, 'C09': 3, 'C10': 4, 'C11': 5,\n","                  'F02': 1, 'F03': 2, 'F04': 3, 'F05': 4, 'F06': 5,\n","                  'F07': 1, 'F08': 2, 'F09': 3, 'F10': 4, 'F11': 5}\n","\n","    df['Repeat'] = df['Well'].map(lambda well: f'{well_order.get(well, 1)}')\n","\n","    # Create the 'Unique_ID' column\n","    df['Unique_ID'] = file_name_without_ext + '_' + df['TRACK_ID'].astype(str)\n","\n","    # Add this DataFrame to the list\n","    df_list.append(df)\n","\n","# Concatenate all the DataFrames together\n","merged_df = pd.concat(df_list, ignore_index=True)\n","\n","# Save the merged DataFrame to a CSV file\n","merged_df.to_csv(os.path.join(Results_Folder, 'merged_Tracks.csv'), index=False)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qonifQDMSZIk","cellView":"form"},"outputs":[],"source":["# @title #Plot and extract useful data\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import os\n","\n","#print(merged_df.columns)\n","#print(merged_df.head())\n","\n","\n","# Assuming that merged_df is your DataFrame\n","# List of variables to plot\n","variables_to_plot = [\"DIVISION_TIME_MEAN\", \"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","#variables_to_plot = [\"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","\n","\n","# Initialize PDF\n","#pdf_pages = PdfPages(Results_Folder +'Boxplots.pdf')\n","\n","# **Change this part:**\n","if not os.path.exists(f\"{Results_Folder}/plots\"):\n","    os.makedirs(f\"{Results_Folder}/plots\")\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'plots', 'Boxplots_raw.pdf') # Save PDF in 'plots' folder\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a single figure with 4 subplots, one for each variable\n","fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(10, 20))\n","\n","for ax, var in zip(axes, variables_to_plot):\n","    # Extract the data for this variable\n","    data_for_var = merged_df[['Well', 'FOV', var]]\n","\n","    # Save this data to a CSV file\n","    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n","\n","    # Sort the 'Group' column alphabetically\n","    group_order = merged_df['Well'].sort_values().unique()\n","\n","    sns.boxplot(x='Well', y=var, data=merged_df, ax=ax, color='lightgray', order=group_order)  # Boxplot\n","    #sns.stripplot(x='Well', y=var, data=merged_df, ax=ax, hue='FOV', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","\n","    ax.set_title(f\"{var}\")\n","    ax.set_xlabel('Well')\n","    ax.set_ylabel(var)\n","\n","if not os.path.exists(f\"{Results_Folder}/plots\"):\n","    os.makedirs(f\"{Results_Folder}/plots\")\n","\n","# Save the figure to a PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)\n","\n","# Close the PDF\n","pdf_pages.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpcNLyU9dy4u","cellView":"form"},"outputs":[],"source":["# @title #Plot and extract useful data and pool conditions\n","\n","#Filter_tracks = 0  # @param {type: \"number\"}\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Assuming that merged_df is your DataFrame\n","# List of variables to plot\n","variables_to_plot = [\"DIVISION_TIME_MEAN\", \"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","#variables_to_plot = [\"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","\n","\n","# Initialize PDF\n","#pdf_pages = PdfPages(Results_Folder+'Boxplots_pooled_CF.pdf')\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'plots', 'Boxplots_pooled_raw.pdf') # Save PDF in 'plots' folder\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a single figure with 4 subplots, one for each variable\n","fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(10, 20))\n","\n","# Create\n","\n","for ax, var in zip(axes, variables_to_plot):\n","    # Extract the data for this variable\n","    data_for_var = merged_df[['Well', 'FOV', 'Condition', var]]\n","\n","\n","    # Save this data to a CSV file\n","    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}.csv\", index=False)\n","\n","    # Sort the 'Group' column alphabetically\n","    group_order = merged_df['Condition'].sort_values().unique()\n","\n","    #sns.boxplot(x='Well', y=var, data=merged_df, ax=ax, color='lightgray')  # Boxplot\n","    #sns.boxplot(x='Group', y=var, data=merged_df, ax=ax, color='lightgray')  # Boxplot\n","    sns.boxplot(x='Condition', y=var, data=merged_df, ax=ax, color='lightgray', order=group_order)\n","    #sns.stripplot(x='Well', y=var, data=merged_df, ax=ax, hue='FOV', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","\n","    ax.set_title(f\"{var}\")\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel(var)\n","\n","# Save the figure to a PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)\n","\n","# Close the PDF\n","pdf_pages.close()"]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for DIVISION_TIME_MEAN\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_DIVISION_TIME_MEAN.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the DIVISION_TIME_MEAN column\n","df = df.dropna(subset=['DIVISION_TIME_MEAN'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['DIVISION_TIME_MEAN'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_DIVISION_TIME_MEAN.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n","\n"],"metadata":{"cellView":"form","id":"1eVbi5XmxmPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for TRACK_MEAN_SPEED\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_TRACK_MEAN_SPEED.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the DIVISION_TIME_MEAN column\n","df = df.dropna(subset=['TRACK_MEAN_SPEED'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['TRACK_MEAN_SPEED'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_TRACK_MEAN_SPEED.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n","\n"],"metadata":{"cellView":"form","id":"eip5oUKWy86l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Check the number of track per condition per repeats\n","\n","def count_tracks_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID'):\n","    \"\"\"\n","    Counts the number of unique tracks for each combination of condition and repeat in the given DataFrame and\n","    saves a stacked histogram plot as a PDF in the QC folder with annotations for each stack.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    Results_Folder (str): The base folder where the results will be saved.\n","    condition_col (str): The name of the column representing the condition. Default is 'Condition'.\n","    repeat_col (str): The name of the column representing the repeat. Default is 'Repeat'.\n","    track_id_col (str): The name of the column representing the track ID. Default is 'Unique_ID'.\n","    \"\"\"\n","    track_counts = df.groupby([condition_col, repeat_col])[track_id_col].nunique()\n","    track_counts_df = track_counts.reset_index()\n","    track_counts_df.rename(columns={track_id_col: 'Number_of_Tracks'}, inplace=True)\n","\n","    # Pivot the data for plotting\n","    pivot_df = track_counts_df.pivot(index=condition_col, columns=repeat_col, values='Number_of_Tracks').fillna(0)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel('Number of Tracks')\n","    ax.set_title('Stacked Histogram of Track Counts per Condition and Repeat')\n","    ax.legend(title=repeat_col)\n","    ax.grid(axis='y', linestyle='--')\n","\n","    # Hide horizontal grid lines\n","    ax.yaxis.grid(False)\n","\n","    # Add number annotations on each stack\n","    for bar in bars.patches:\n","        ax.text(bar.get_x() + bar.get_width() / 2,\n","                bar.get_y() + bar.get_height() / 2,\n","                int(bar.get_height()),\n","                ha='center', va='center', color='black', fontweight='bold', fontsize=8)\n","\n","    # Save the plot as a PDF\n","    pdf_file = os.path.join(Results_Folder, 'Track_Counts_Histogram.pdf')\n","    plt.savefig(pdf_file, bbox_inches='tight')\n","    print(f\"Saved histogram to {pdf_file}\")\n","\n","    plt.show()\n","\n","    return track_counts_df\n","\n","\n","if not os.path.exists(f\"{Results_Folder}/QC\"):\n","    os.makedirs(f\"{Results_Folder}/QC\")\n","\n","result_df = count_tracks_by_condition_and_repeat(merged_df, f\"{Results_Folder}/QC\")\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"1F2dyKH55VXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Run this cell to downsample and balance your dataset\n","\n","!pip install tqdm  # Install the tqdm module\n","from tqdm import tqdm # import the tqdm function\n","import gzip # import gzip to read and write gzipped file\n","\n","def balance_dataset(df, condition_col='Condition', repeat_col='Repeat', track_id_col='Unique_ID', random_seed=None):\n","    \"\"\"\n","    Balances the dataset by downsampling tracks for each condition and repeat combination.\n","\n","    Parameters:\n","    df (pandas.DataFrame): The DataFrame containing the data.\n","    condition_col (str): The name of the column representing the condition.\n","    repeat_col (str): The name of the column representing the repeat.\n","    track_id_col (str): The name of the column representing the track ID.\n","    random_seed (int, optional): The seed for the random number generator. Default is None.\n","\n","    Returns:\n","    pandas.DataFrame: A new DataFrame with balanced track counts.\n","    \"\"\"\n","    # Group by condition and repeat, and find the minimum track count\n","    min_track_count = df.groupby([condition_col, repeat_col])[track_id_col].nunique().min()\n","\n","    # Function to sample min_track_count tracks from each group\n","    def sample_tracks(group):\n","        return group.sample(n=min_track_count, random_state=random_seed)\n","\n","    # Apply sampling to each group and concatenate the results\n","    balanced_merged_tracks_df = df.groupby([condition_col, repeat_col]).apply(sample_tracks).reset_index(drop=True)\n","\n","    return balanced_merged_tracks_df\n","\n","def replace_inf_with_nan(df, df_name):\n","    \"\"\"\n","    Replaces all infinite values (positive or negative infinity) in the DataFrame with NaN\n","    and prints a message for each column where infinities are found.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to replace inf values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","\n","    Returns:\n","    pd.DataFrame: DataFrame with infinity values replaced by NaN.\n","    \"\"\"\n","    # Check for positive and negative infinity\n","    inf_columns = df.columns[(df == np.inf).any() | (df == -np.inf).any()].tolist()\n","\n","    # Print message for each column that contains infinity values\n","    if inf_columns:\n","        for col in inf_columns:\n","            inf_count = ((df[col] == np.inf) | (df[col] == -np.inf)).sum()\n","            print(f\"Column '{col}' in {df_name} contains {inf_count} infinity values. Replacing with NaN.\")\n","\n","    # Replace inf and -inf with NaN and update the DataFrame in place\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    return df  # Return the modified DataFrame\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","    It first converts infinite values to NaNs before the check.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    # Replace infinity with NaN before checking for NaN values\n","    df = replace_inf_with_nan(df, df_name)\n","\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar and gzip compression.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing with gzip compression\n","        with gzip.open(path, \"wt\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","random_seed = 42\n","\n","if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n","    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n","\n","balanced_merged_tracks_df = balance_dataset(merged_df, random_seed=random_seed)\n","result_df = count_tracks_by_condition_and_repeat(balanced_merged_tracks_df, f\"{Results_Folder}/Balanced_dataset\")\n","\n","check_for_nans(balanced_merged_tracks_df, \"balanced_merged_tracks_df\")\n","save_dataframe_with_progress(balanced_merged_tracks_df, Results_Folder + '/Balanced_dataset/merged_Tracks_balanced_dataset.csv.gz')"],"metadata":{"cellView":"form","id":"-l9N94Zb6QMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot and extract useful data and pool conditions (balanced)\n","\n","#Filter_tracks = 0  # @param {type: \"number\"}\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Initialize PDF2 - Only once!\n","pdf_path = os.path.join(Results_Folder, 'plots', 'Boxplots_pooled_balanced.pdf')\n","pdf_pages = PdfPages(pdf_path)  # This creates the pdf object\n","\n","# Assuming that merged_df is your DataFrame\n","# List of variables to plot\n","variables_to_plot = [\"DIVISION_TIME_MEAN\", \"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","#variables_to_plot = [\"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","\n","\n","# Create a single figure with 4 subplots, one for each variable\n","fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(10, 20))\n","\n","for ax, var in zip(axes, variables_to_plot):\n","    # Extract the data for this variable\n","    data_for_var = balanced_merged_tracks_df[['Well', 'FOV', 'Condition', var]]\n","\n","\n","    # Save this data to a CSV file\n","    data_for_var.to_csv(f\"{Results_Folder}/data_for_{var}_balanced.csv\", index=False)\n","\n","    # Sort the 'Group' column alphabetically\n","    group_order = balanced_merged_tracks_df['Condition'].sort_values().unique()\n","\n","    sns.boxplot(x='Condition', y=var, data=balanced_merged_tracks_df, ax=ax, color='lightgray', order=group_order)\n","\n","    ax.set_title(f\"{var}\")  # Set title for each subplot\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel(var)\n","\n","# Save the figure to a PDF - Only once, outside the loop\n","plt.tight_layout()\n","pdf_pages.savefig(fig)\n","\n","# Close the PDF -  Important to close after saving\n","pdf_pages.close()"],"metadata":{"cellView":"form","id":"Rh3vd5Ka6x9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot and extract useful data and pool conditions with repeats (balanced)\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","import os\n","\n","# Initialize PDF2\n","pdf_path = os.path.join(Results_Folder, 'Boxplots_pooled_balanced_repeats.pdf')\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Assuming 'balanced_merged_tracks_df' is your DataFrame\n","# List of variables to plot\n","variables_to_plot = [\"DIVISION_TIME_MEAN\", \"TRACK_MEAN_SPEED\", \"TOTAL_DISTANCE_TRAVELED\", \"TRACK_DURATION\"]\n","\n","# Initialize PDF - **This line was causing the error, it's fixed below**\n","pdf_path = os.path.join(Results_Folder, 'plots',  'Boxplots_pooled_balanced_repeat.pdf')  # Use Results_Folder variable here\n","pdf_pages = PdfPages(pdf_path)\n","\n","# Create a single figure with subplots for each variable\n","fig, axes = plt.subplots(len(variables_to_plot), 1, figsize=(12, 24))\n","\n","# Loop over variables and plot\n","for ax, var in zip(axes, variables_to_plot):\n","    # Extract data for the variable\n","    data_for_var = balanced_merged_tracks_df[['Well', 'FOV', 'Condition', 'Repeat', var]]\n","\n","    # Corrected line for saving the CSV\n","    #data_for_var.to_csv(os.path.join(Results_Folder, f\"data_for_{var}_balanced.csv\"), index=False)\n","\n","\n","    # Determine the order of the conditions\n","    group_order = balanced_merged_tracks_df['Condition'].sort_values().unique()\n","\n","    # Create the boxplot for pooled data\n","    sns.boxplot(\n","        x='Condition',\n","        y=var,\n","        data=balanced_merged_tracks_df,\n","        ax=ax,\n","        color='lightgray',\n","        order=group_order\n","    )\n","\n","    # Overlay with a stripplot showing individual repeats\n","    sns.stripplot(\n","        x='Condition',\n","        y=var,\n","        hue='Repeat',\n","        data=balanced_merged_tracks_df,\n","        ax=ax,\n","        dodge=True,\n","        jitter=True,\n","        palette='magma',\n","        alpha=0.5,\n","        order=group_order\n","    )\n","\n","    # Customize plot labels and legend\n","    ax.set_title(f\"{var}\")\n","    ax.set_xlabel('Condition')\n","    ax.set_ylabel(var)\n","    ax.legend(title='Repeat', bbox_to_anchor=(1.05, 1), loc='upper left')\n","\n","# Adjust layout and save to PDF\n","plt.tight_layout()\n","pdf_pages.savefig(fig)\n","pdf_pages.close()"],"metadata":{"id":"jQeDzzPS8jWC","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for DIVISION_TIME_MEAN_balanced\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_DIVISION_TIME_MEAN_balanced.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the DIVISION_TIME_MEAN column\n","df = df.dropna(subset=['DIVISION_TIME_MEAN'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['DIVISION_TIME_MEAN'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_DIVISION_TIME_MEAN_balanced.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n","\n","\n","\n","\n","\n"],"metadata":{"id":"oD0rEj1iFoM7","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Calculate statistics between conditions for TRACK_MEAN_SPEED_balanced\n","\n","# @title #Calculate statistics between conditions for TRACK_MEAN_SPEED\n","\n","import pandas as pd\n","from scipy.stats import ttest_ind\n","from numpy import std\n","import os\n","\n","# Load the CSV file into a pandas DataFrame\n","file_path = os.path.join(Results_Folder, 'data_for_TRACK_MEAN_SPEED_balanced.csv')\n","df = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in the DIVISION_TIME_MEAN column\n","df = df.dropna(subset=['TRACK_MEAN_SPEED'])\n","\n","# Define the specific conditions to compare\n","comparisons = [\n","    ('Control pool', 'Mutation #34'),\n","    ('Control pool', 'Mutation #38'),\n","    ('Control single cell', 'Mutation #34'),\n","    ('Control single cell', 'Mutation #38')\n","]\n","\n","# Create a dictionary to store condition data\n","condition_groups = {condition: df[df['Condition'] == condition]['TRACK_MEAN_SPEED'] for condition in df['Condition'].unique()}\n","\n","# Display group lengths\n","for condition, data in condition_groups.items():\n","    print(f'{condition} length: {len(data)}')\n","\n","# Perform specified t-tests and Cohen's d calculations\n","t_statistics = []\n","p_values = []\n","cohen_ds = []\n","\n","def cohen_d(group_a, group_b):\n","    mean_diff = group_a.mean() - group_b.mean()\n","    pooled_std = std(pd.concat([group_a, group_b], axis=0), ddof=1)\n","    return mean_diff / pooled_std\n","\n","comparison_labels = []\n","for cond_a, cond_b in comparisons:\n","    if cond_a in condition_groups and cond_b in condition_groups:\n","        group_a, group_b = condition_groups[cond_a], condition_groups[cond_b]\n","\n","        t_stat, p_value = ttest_ind(group_a, group_b)\n","        d_value = cohen_d(group_a, group_b)\n","\n","        comparison_labels.append(f'{cond_a} vs {cond_b}')\n","        t_statistics.append(t_stat)\n","        p_values.append(p_value)\n","        cohen_ds.append(d_value)\n","    else:\n","        print(f'Skipping comparison {cond_a} vs {cond_b} due to missing data.')\n","\n","# Create a DataFrame for the results\n","results_df = pd.DataFrame({\n","    'Comparison': comparison_labels,\n","    'T-statistic': t_statistics,\n","    'P-value': p_values,\n","    \"Cohen's d\": cohen_ds\n","})\n","\n","# Export results to a CSV file with p-values formatted to 5 decimal places\n","results_csv_path = os.path.join(Results_Folder, 't_test_results_TRACK_MEAN_SPEED_balanced.csv')\n","results_df.to_csv(results_csv_path, index=False, float_format='%.5f')\n","\n","# Print the results\n","print('T-test results:')\n","print(results_df)\n","print(f'Results exported to: {results_csv_path}')\n"],"metadata":{"id":"vrwU-udIGCdi","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# READY!!!"],"metadata":{"id":"izBCEOhNHe9I"}}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}