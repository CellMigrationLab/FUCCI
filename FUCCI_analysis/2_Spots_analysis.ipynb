{"cells":[{"cell_type":"markdown","source":["# **Spots analysis for csv files from TrackMate**\n","\n","This notebook measures:\n","\n","*   peak count (ch2)\n","*   peak distanced\n","* full withd of hal maximum (fwhm) of peaks\n","\n","Written by Joanna Pylvänäinen\n","\n","joanna.pylvanainen@abo.fi"],"metadata":{"id":"OECNsYHOz7nU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzebO-DQ7fp_","cellView":"form"},"outputs":[],"source":["# @title #Mount GDrive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install fastdtw\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"00-bB41W1jzx"},"outputs":[],"source":["# @title #Load useful functions\n","\n","\n","def normalize_to_01(series):\n","    \"\"\"\n","    Normalize a given time series (1D numpy array) between 0 and 1.\n","\n","    Parameters:\n","        series (numpy.ndarray): The time series to be normalized.\n","\n","    Returns:\n","        numpy.ndarray: The normalized time series.\n","    \"\"\"\n","    Imin = np.min(series)\n","    Imax = np.max(series)\n","\n","    if Imax == Imin:\n","        return np.zeros_like(series)\n","\n","    return (series - Imin) / (Imax - Imin)\n","\n","# Function to normalize by maximum amplitude\n","def normalize_by_max_amplitude(series):\n","    return series / np.max(np.abs(series))"]},{"cell_type":"markdown","metadata":{"id":"j1q7L5au1NV7"},"source":["\n","\n","---\n","\n","\n","# **Part 2: Analysing spot data**\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFLimAMXaz8g","cellView":"form"},"outputs":[],"source":["# @title # Load spots and tracks tables\n","import pandas as pd\n","import glob\n","import os\n","import numpy as np\n","\n","# Paths\n","Data_folder_path = '/content/drive/MyDrive/Kurppa/2025_reanalysis/Drifty_removed/1FUCCI_Osimertinib_tracking_images_frame48-132_no_split'  # @param {type: \"string\"}\n","Results_Folder = \"/content/drive/MyDrive/Kurppa/2025_reanalysis/Cleaned_notebooks/results_spots\"  # @param {type: \"string\"}\n","\n","# Function to extract metadata and process the DataFrame\n","def process_dataframe(filepath, well_info, fov_info, file_name_without_ext):\n","    metadata = {\n","        'Well': well_info,\n","        'FOV': fov_info,\n","        'File Name': file_name_without_ext\n","    }\n","\n","    metadata['Condition'] = np.select(\n","        [well_info in ['C02', 'C03', 'C04', 'C05', 'C06'],\n","         well_info in ['C07', 'C08', 'C09', 'C10', 'C11'],\n","         well_info in ['F02', 'F03', 'F04', 'F05', 'F06'],\n","         well_info in ['F07', 'F08', 'F09', 'F10', 'F11']],\n","        ['Control pool', 'Control single cell', 'Mutation #34', 'Mutation #38'],\n","        default='Unknown'\n","    )\n","\n","    well_order = {'C02': 1, 'C03': 2, 'C04': 3, 'C05': 4, 'C06': 5,\n","                  'C07': 1, 'C08': 2, 'C09': 3, 'C10': 4, 'C11': 5,\n","                  'F02': 1, 'F03': 2, 'F04': 3, 'F05': 4, 'F06': 5,\n","                  'F07': 1, 'F08': 2, 'F09': 3, 'F10': 4, 'F11': 5}\n","\n","    metadata['Repeat'] = f'{well_order.get(well_info, 1)}'\n","\n","    return metadata\n","\n","# Process spots data\n","spots_df_list = []\n","for filepath in glob.glob(Data_folder_path + '/*spots*.csv'):\n","    filename = os.path.basename(filepath)\n","    well_info = filename[0:3]\n","    fov_info = filename.split('_')[1]\n","    file_name_without_ext = os.path.splitext(filename)[0].replace('_tracking-spots', '')\n","\n","    df = pd.read_csv(filepath, skiprows=[1, 2, 3])\n","\n","    metadata = process_dataframe(filepath, well_info, fov_info, file_name_without_ext)\n","    for key, value in metadata.items():\n","        df[key] = value\n","\n","    df['Unique_ID'] = file_name_without_ext + '_' + df['TRACK_ID'].astype(str)\n","\n","    if 'TOTAL_INTENSITY_CH1' in df.columns and 'TOTAL_INTENSITY_CH2' in df.columns:\n","        df['Yellow'] = df['TOTAL_INTENSITY_CH1'] + df['TOTAL_INTENSITY_CH2']\n","\n","    spots_df_list.append(df)\n","\n","# Process tracks data\n","tracks_df_list = []\n","for filepath in glob.glob(Data_folder_path + '/*tracks*.csv'):\n","    filename = os.path.basename(filepath)\n","    well_info = filename[0:3]\n","    fov_info = filename.split('_')[1]\n","    file_name_without_ext = os.path.splitext(filename)[0].replace('_tracking-tracks', '')\n","\n","    df = pd.read_csv(filepath, skiprows=[1, 2, 3])\n","\n","    metadata = process_dataframe(filepath, well_info, fov_info, file_name_without_ext)\n","    for key, value in metadata.items():\n","        df[key] = value\n","\n","    df['Unique_ID'] = file_name_without_ext + '_' + df['TRACK_ID'].astype(str)\n","\n","    tracks_df_list.append(df)\n","\n","# Merge all data\n","merged_spots_df = pd.concat(spots_df_list, ignore_index=True)\n","merged_tracks_df = pd.concat(tracks_df_list, ignore_index=True)\n","\n","# Select relevant columns from merged_tracks_df for merging\n","tracks_info = merged_tracks_df[['Unique_ID', 'TRACK_DURATION', 'NUMBER_SPOTS']]\n","\n","# Merge TRACK_DURATION and NUMBER_SPOTS into merged_spots_df based on Unique_ID\n","merged_spots_df = merged_spots_df.merge(tracks_info, on='Unique_ID', how='left')\n","\n","# Save the final CSVs\n","merged_spots_df.to_csv(Results_Folder + '/' + 'merged_Spots.csv', index=False)\n","merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n","\n","print(\"Spots and Tracks files merged and saved successfully.\")\n"]},{"cell_type":"markdown","metadata":{"id":"dZgTvz3R31Qe"},"source":["# **Part 2.1: Peak Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwaJ6EdPE9y_","cellView":"form"},"outputs":[],"source":["paramater_to_plot = 'TOTAL_INTENSITY_CH2'\n","\n","#TOTAL_INTENSITY_CH1 TOTAL_INTENSITY_CH2 Yellow\n","\n","# @title #Prepare and normalise the data (Fluo over time) of balanced dataset (example well F04_02)\n","\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Your normalize_to_01 function\n","def normalize_to_01(series):\n","    return (series - series.min()) / (series.max() - series.min())\n","\n","\n","# Filter data for the specific well \"F02\" and FOV \"02\"\n","filtered_df = merged_spots_df[(merged_spots_df['Well'] == 'F04') & (merged_spots_df['FOV'] == '02')]\n","filtered_df = filtered_df.groupby('TRACK_ID').filter(lambda x: len(x) >= 50)\n","\n","# Group by 'TRACK_ID'\n","grouped = filtered_df.groupby('TRACK_ID')\n","valid_track_ids = [name for name, group in grouped if group[paramater_to_plot].max() > 10000]\n","\n","# Filter DataFrame for valid TRACK_IDs\n","filtered_df = filtered_df[filtered_df['TRACK_ID'].isin(valid_track_ids)]\n","\n","# Sort by 'TRACK_ID' and 'POSITION_T'\n","filtered_df.sort_values(['TRACK_ID', 'POSITION_T'], inplace=True)\n","\n","# Rolling average\n","window_size = 3\n","filtered_df['Rolling_Avg'] = filtered_df.groupby('TRACK_ID')[paramater_to_plot].transform(\n","    lambda x: x.rolling(window=window_size).mean()\n",")\n","\n","# Remove NaNs\n","filtered_df.dropna(subset=['Rolling_Avg'], inplace=True)\n","\n","# Normalization\n","for track_id in filtered_df['TRACK_ID'].unique():\n","    track_data = filtered_df[filtered_df['TRACK_ID'] == track_id]\n","    filtered_df.loc[filtered_df['TRACK_ID'] == track_id, 'Rolling_Avg_normalised'] = normalize_to_01(track_data['Rolling_Avg'])\n","\n","# Get 10 unique track IDs for plotting\n","selected_track_ids = filtered_df['TRACK_ID'].unique()[:10]\n","\n","# Define the new folder name\n","new_folder = os.path.join(Results_Folder, 'normalized_track_data_plots')\n","\n","# Create the new folder if it doesn't exist\n","os.makedirs(new_folder, exist_ok=True)\n","\n","# Define the new folder name\n","new_folder = os.path.join(Results_Folder, 'normalized_track_data_plots')\n","os.makedirs(new_folder, exist_ok=True)\n","pdf_pages = PdfPages(os.path.join(new_folder, 'Track_Data_Plots_example well F04_02.pdf'))\n","\n","# Create plots for the selected tracks\n","for track_id in selected_track_ids:\n","    track_data = filtered_df[filtered_df['TRACK_ID'] == track_id]\n","\n","    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    # Plot raw data\n","    sns.lineplot(x='POSITION_T', y=paramater_to_plot, data=track_data, ax=axs[0])\n","    axs[0].set_title(f'Track {track_id} - Raw Data')\n","\n","    # Plot rolling average\n","    sns.lineplot(x='POSITION_T', y='Rolling_Avg', data=track_data, ax=axs[1])\n","    axs[1].set_title(f'Track {track_id} - Rolling Avg')\n","\n","    # Plot normalized rolling average\n","    sns.lineplot(x='POSITION_T', y='Rolling_Avg_normalised', data=track_data, ax=axs[2])\n","    axs[2].set_title(f'Track {track_id} - Normalized')\n","\n","    plt.tight_layout()\n","    pdf_pages.savefig(fig)\n","    plt.show()\n","    plt.close(fig)\n","\n","pdf_pages.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIxQY4FGiioA","cellView":"form"},"outputs":[],"source":["# @title #Align peaks\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial.distance import euclidean\n","from fastdtw import fastdtw\n","import matplotlib.pyplot as plt\n","from scipy.interpolate import interp1d\n","from scipy.signal import find_peaks\n","\n","\n","def evaluate_track(track_data):\n","    mean_intensity = np.mean(track_data)\n","    std_intensity = np.std(track_data)\n","    snr = mean_intensity / std_intensity if std_intensity != 0 else 0\n","    # Peak detection\n","    peaks, _ = find_peaks(track_data, height=0.5)  # Adjust height parameter as needed\n","    # Regularity (standard deviation of the distance between peaks)\n","    peak_distances = np.diff(peaks)\n","    regularity = np.std(peak_distances) if len(peak_distances) > 0 else 0\n","    return snr, len(peaks), regularity\n","\n","# Get unique track IDs\n","unique_tracks = filtered_df['TRACK_ID'].unique()\n","\n","# Evaluate all tracks\n","track_metrics = {}\n","for track_id in unique_tracks:\n","    track_data = filtered_df[filtered_df['TRACK_ID'] == track_id]['Rolling_Avg_normalised']\n","    snr, num_peaks, regularity = evaluate_track(track_data)\n","    track_metrics[track_id] = {'SNR': snr, 'NumPeaks': num_peaks, 'Regularity': regularity}\n","\n","# Convert dict to DataFrame for easier manipulation\n","track_metrics_df = pd.DataFrame.from_dict(track_metrics, orient='index')\n","\n","# Find the \"best\" track based on the metrics\n","best_tracks_df = track_metrics_df.loc[\n","    (track_metrics_df['NumPeaks'] == track_metrics_df['NumPeaks'].max())]\n","\n","# Check if the DataFrame is empty\n","if not best_tracks_df.empty:\n","    best_track_id = best_tracks_df.index[0]\n","    print(\"Best Track ID:\", best_track_id)\n","else:\n","    print(\"No track meets all the criteria.\")\n","\n","# Find the longest series\n","lengths = [len(filtered_df[filtered_df['TRACK_ID'] == track]) for track in unique_tracks]\n","longest_track = unique_tracks[np.argmax(lengths)]\n","\n","# Initialize a list to store aligned time series\n","aligned_series_list = []\n","\n","# Choose a reference track, you can change this\n","reference_track = longest_track\n","reference_df = filtered_df[filtered_df['TRACK_ID'] == best_track_id]\n","reference_series = reference_df[['POSITION_T', 'Rolling_Avg_normalised']].values\n","\n","# Align all other tracks to the reference track\n","for track in unique_tracks[1:]:\n","    current_df = filtered_df[filtered_df['TRACK_ID'] == track]\n","    current_series = current_df[['POSITION_T', 'Rolling_Avg_normalised']].values\n","\n","    # Perform DTW\n","    _, path = fastdtw(reference_series, current_series, dist=euclidean)\n","\n","    # Extract the aligned indices\n","    idx_ref, idx_current = zip(*path)\n","\n","    # Interpolate to match the reference series length\n","    interp_func = interp1d(current_series[list(idx_current), 0], current_series[list(idx_current), 1], kind='linear', fill_value=\"extrapolate\")\n","    aligned_series = interp_func(reference_series[:, 0])\n","\n","\n","    # Store aligned time series\n","    aligned_series_list.append(aligned_series)\n","\n","# Convert the list of aligned series into a 2D array\n","aligned_series_array = np.array(aligned_series_list)\n","\n","# Plot the reference series and a few aligned series\n","plt.figure(figsize=(10, 6))\n","plt.plot(reference_series[:, 1], label=f'Reference {reference_track}')\n","\n","# Plot first 5 aligned series\n","for i, aligned_series in enumerate(aligned_series_array[:15]):\n","    plt.plot(aligned_series, label=f'Aligned {unique_tracks[i + 1]}')  # i+1 because the reference track is not in aligned_series_array\n","\n","plt.legend()\n","plt.title(paramater_to_plot)\n","plt.xlabel('Aligned Time')\n","plt.ylabel(paramater_to_plot)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7ga0aM9YHHf","cellView":"form"},"outputs":[],"source":["# @title #Average aligned peaks\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Convert the list of aligned series into a 2D array\n","aligned_series_array = np.array(aligned_series_list)\n","\n","# Find the minimum length among all series in the array\n","min_length = min([len(series) for series in aligned_series_array])\n","\n","# Crop each time series to this minimum length\n","cropped_series_array = np.array([series[:min_length] for series in aligned_series_array])\n","\n","# Calculate mean and standard deviation along the rows\n","mean_series = np.mean(cropped_series_array, axis=0)\n","std_series = np.std(cropped_series_array, axis=0)\n","\n","# Plot mean and standard deviation\n","plt.figure(figsize=(10, 6))\n","plt.plot(mean_series, label='Mean', color='blue')\n","plt.fill_between(range(len(mean_series)), mean_series-std_series, mean_series+std_series,\n","                 color='blue', alpha=0.2, label='Standard Deviation')\n","plt.title('Mean and Standard Deviation of Aligned and Normalized Time Series')\n","plt.xlabel('Aligned Time')\n","plt.ylabel(paramater_to_plot)\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clvPH74ccvrY","cellView":"form"},"outputs":[],"source":["# @title #Find the distance between peaks for selected condition\n","\n","import pandas as pd\n","from scipy.signal import find_peaks\n","import numpy as np\n","\n","# Initialize an empty dictionary to store peak distances for each TRACK_ID\n","peak_distances_dict = {}\n","\n","# Loop through each unique TRACK_ID\n","for track_id in filtered_df['TRACK_ID'].unique():\n","\n","    # Filter the DataFrame to get data for this specific TRACK_ID\n","    track_df = filtered_df[filtered_df['TRACK_ID'] == track_id]\n","\n","    # Extract the Rolling_Avg and POSITION_T values\n","    rolling_avg_values = track_df['Rolling_Avg_normalised'].values\n","    position_t_values = track_df['POSITION_T'].values\n","\n","    # Find peaks in the Rolling_Avg data\n","    peaks, _ = find_peaks(rolling_avg_values, height=0.8, distance=10)\n","\n","    if len(peaks) < 2:  # Skip tracks with fewer than two peaks\n","        continue\n","\n","    # Calculate the distances (in terms of POSITION_T) between consecutive peaks\n","    peak_positions = position_t_values[peaks]\n","    peak_distances = np.diff(peak_positions)\n","\n","    # Store these distances in the dictionary\n","    peak_distances_dict[track_id] = peak_distances\n","\n","# Convert the dictionary to a DataFrame for easier manipulation\n","peak_distances_df = pd.DataFrame.from_dict(peak_distances_dict, orient='index')\n","peak_distances_df.columns = [f'Distance_{i+1}' for i in range(peak_distances_df.shape[1])]\n","\n","# Calculate the average peak distance for each track and store it in a new DataFrame\n","average_peak_distances = peak_distances_df.mean(axis=1).reset_index()\n","average_peak_distances.columns = ['TRACK_ID', 'Average_Peak_Distance']\n","\n","print(average_peak_distances)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iwCW566Ap91","cellView":"form"},"outputs":[],"source":["# @title #Measure Peak duration\n","\n","threshold = 0.5  # Set your threshold value here\n","\n","\n","def find_FWTM(x, y, threshold):\n","    max_y = max(y)\n","    threshold_y = max_y * threshold\n","    left_idx = np.where(y >= threshold_y)[0][0]\n","    right_idx = np.where(y >= threshold_y)[0][-1]\n","    FWTM = x[right_idx] - x[left_idx]\n","    return FWTM, x[left_idx], x[right_idx]\n","\n","fwhm_dict = {}\n","\n","# Loop through each unique TRACK_ID\n","num_plots = 0  # Number of plots generated\n","\n","for track_id in filtered_df['TRACK_ID'].unique():  # Process all unique TRACK_IDs\n","\n","    # Filter the DataFrame to get data for this specific TRACK_ID\n","    track_df = filtered_df[filtered_df['TRACK_ID'] == track_id]\n","\n","    if num_plots >= 10:\n","        break  # Stop after plotting 10 tracks\n","\n","    # Extract the Rolling_Avg and POSITION_T values\n","    rolling_avg_values = track_df['Rolling_Avg_normalised'].values\n","    position_t_values = track_df['POSITION_T'].values\n","\n","    # Find peaks in the Rolling_Avg data\n","    peaks, _ = find_peaks(rolling_avg_values, height=0.5, distance=10)\n","\n","    fwhm_list = []\n","\n","    plt.figure()\n","    plt.plot(position_t_values, rolling_avg_values, label=\"Signal\")\n","\n","    for peak in peaks:\n","\n","        start_idx = np.where(rolling_avg_values[:peak] < threshold)[0][-1] if np.any(rolling_avg_values[:peak] < threshold) else 0\n","        end_idx = np.where(rolling_avg_values[peak:] < threshold)[0][0] + peak if np.any(rolling_avg_values[peak:] < threshold) else len(rolling_avg_values) - 1\n","\n","        # Update local_x and local_y based on threshold\n","        local_x = position_t_values[start_idx:end_idx]\n","        local_y = rolling_avg_values[start_idx:end_idx]\n","\n","        FWHM, left, right = find_FWTM(local_x, local_y, 0.4)  # Here, the threshold is 0.5, i.e., half maximum\n","\n","        fwhm_list.append(FWHM)\n","\n","        plt.scatter(position_t_values[peak], rolling_avg_values[peak], color='red')  # Mark the peak\n","        plt.axvline(x=left, color='green', linestyle='--')  # Mark the left half-maximum\n","        plt.axvline(x=right, color='green', linestyle='--')  # Mark the right half-maximum\n","\n","    plt.title(f'Track {track_id}')\n","    plt.xlabel('POSITION_T')\n","    plt.ylabel('Rolling Avg Intensity')\n","    plt.legend()\n","    plt.show()\n","\n","    # Save the list of FWHM values for this track to the dictionary\n","    fwhm_dict[track_id] = fwhm_list\n","\n","    num_plots += 1  # Increment the number of plots generated\n","\n","# Convert the dictionary to a DataFrame for easier manipulation\n","fwhm_df = pd.DataFrame.from_dict(fwhm_dict, orient='index')\n","fwhm_df.columns = [f'FWHM_{i+1}' for i in range(fwhm_df.shape[1])]"]},{"cell_type":"markdown","metadata":{"id":"lW4RLRbw34at"},"source":["# **Part 2.2: Analyse all conditions**"]},{"cell_type":"code","source":["# @title #Measure distance between two G1 peaks for all conditions (this can take up to 1h depending on the size of data)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Move these to the top of the script as configuration parameters\n","PEAK_DETECTION_CONFIG = {\n","    'height': 10000,  # Peak_height\n","    'distance': 5,    # Peak_distance\n","    'rolling_window': 3,  # window_size\n","    'signal_background': 5000,  # Signal_background\n","    'fwtm_threshold': 0.4  # Threshold for FWTM calculation\n","}\n","\n","paramater_to_plot = 'TOTAL_INTENSITY_CH2'\n","window_size = 3  # for rolling average\n","Signal_background = 5000  # for filtering\n","Peak_height = 10000  # for peak detection\n","Peak_distance = 5\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.signal import find_peaks\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","threshold = 5000  # Set your threshold value here\n","\n","fwhm_dict = {}\n","peak_distances_dict = {}\n","\n","def find_FWTM(x, y, threshold):\n","    max_y = max(y)\n","    threshold_y = max_y * threshold\n","    left_idx = np.where(y >= threshold_y)[0][0]\n","    right_idx = np.where(y >= threshold_y)[0][-1]\n","    FWTM = x[right_idx] - x[left_idx]\n","    return FWTM, x[left_idx], x[right_idx]\n","\n","\n","def normalize_to_01(series):\n","    return (series - series.min()) / (series.max() - series.min())\n","\n","# Get unique wells, FOVs, Conditions, and Repeats\n","unique_wells = merged_spots_df['Well'].unique()\n","unique_fovs = merged_spots_df['FOV'].unique()\n","unique_conditions = merged_spots_df['Condition'].unique()\n","unique_repeats = merged_spots_df['Repeat'].unique()\n","\n","# Initialize an empty DataFrame to store the overall results\n","final_results_fwhm = pd.DataFrame()\n","final_results_average_peak_distances = pd.DataFrame()\n","peak_info_df = pd.DataFrame(columns=['Unique_ID', 'Well', 'FOV', 'Has_Peak'])\n","\n","# Initialize a dictionary to store peak counts\n","peak_counts_dict = {}\n","\n","# Loop through each well and each FOV\n","for well in unique_wells:\n","    for fov in unique_fovs:\n","        print(well + fov + \" is being processed\")\n","\n","        # Create the peak_plots folder if it doesn't exist\n","        peak_plots_folder = os.path.join(Results_Folder, \"peak_plots\")\n","        os.makedirs(peak_plots_folder, exist_ok=True)\n","\n","        # Include the peak_plots folder in the PDF file path\n","        pdf_file_path = os.path.join(\n","            peak_plots_folder, f\"{well}_{fov}_All_Tracks_Peaks.pdf\")\n","\n","        # Filter data for the specific well and FOV\n","        filtered_df = merged_spots_df[(merged_spots_df['Well'] == well) & (\n","            merged_spots_df['FOV'] == fov)]\n","        filtered_df = filtered_df.groupby('TRACK_ID').filter(\n","            lambda x: len(x) >= 50)\n","\n","        # Group by 'TRACK_ID'\n","        grouped = filtered_df.groupby('TRACK_ID')\n","        valid_track_ids = [name for name, group in grouped if group[paramater_to_plot].max() > 10000]\n","\n","        # Filter DataFrame for valid TRACK_IDs\n","        filtered_df = filtered_df[filtered_df['TRACK_ID'].isin(valid_track_ids)]\n","\n","        with PdfPages(pdf_file_path) as pdf_pages:  # The with statement\n","\n","            # Sort by 'TRACK_ID' and 'POSITION_T'\n","            filtered_df.sort_values(['TRACK_ID', 'POSITION_T'], inplace=True)\n","\n","            # Apply the rolling mean within each 'TRACK_ID' group\n","            filtered_df['Rolling_Avg'] = filtered_df.groupby('TRACK_ID')[paramater_to_plot].transform(\n","                lambda x: x.rolling(window=window_size).mean())\n","\n","            # Optional: Remove rows with NaN (these are the rows where the rolling mean could not be computed)\n","            filtered_df.dropna(subset=['Rolling_Avg'], inplace=True)\n","\n","            # Normalization\n","            for track_id in filtered_df['TRACK_ID'].unique():\n","                track_data = filtered_df[filtered_df['TRACK_ID']\n","                                          == track_id]\n","                filtered_df.loc[filtered_df['TRACK_ID'] == track_id, 'Rolling_Avg_normalised'] = normalize_to_01(\n","                    track_data['Rolling_Avg'])\n","\n","        # Get unique TRACK_IDs\n","        print(well + fov +\n","              \" Data preprocessed and normalised, looking for peaks\")\n","        unique_track_ids = filtered_df['TRACK_ID'].unique()\n","\n","        # Sort by 'TRACK_ID' and 'POSITION_T' if not already sorted\n","        filtered_df.sort_values(['TRACK_ID', 'POSITION_T'], inplace=True)\n","\n","        # Loop through each unique TRACK_ID\n","        for track_id in filtered_df['Unique_ID'].unique():\n","            # Filter the DataFrame to get data for this specific TRACK_ID\n","            track_df = filtered_df[filtered_df['Unique_ID'] == track_id]\n","\n","            # Get the repeat for the current track_id\n","            repeat = track_df['Repeat'].iloc[0]\n","\n","            # Extract the Rolling_Avg and POSITION_T values\n","            rolling_avg_values = track_df['Rolling_Avg'].values\n","            position_t_values = track_df['POSITION_T'].values\n","            # Find peaks in the Rolling_Avg data\n","            peaks, _ = find_peaks(\n","                rolling_avg_values, height=Peak_height, distance=Peak_distance)\n","\n","            # Count the number of peaks\n","            num_peaks = len(peaks)\n","            peak_counts_dict[track_id] = num_peaks\n","\n","            has_peak = len(peaks) > 0\n","\n","            # Append this information to peak_info_df\n","            peak_info_df = pd.concat([peak_info_df, pd.DataFrame([{\n","                'Unique_ID': track_id,\n","                'Well': well,\n","                'FOV': fov,\n","                'Has_Peak': has_peak,\n","                'Repeat': repeat # Add Repeat here\n","            }])], ignore_index=True)\n","\n","            fwhm_list = []\n","            fig, ax = plt.subplots()\n","            ax.plot(position_t_values, rolling_avg_values, label=\"Signal\")\n","\n","            for peak in peaks:\n","                # Find the neighborhood around the peak\n","                start_idx = peak\n","                end_idx = peak\n","\n","                # Find the left boundary where the signal drops below the threshold\n","                while start_idx > 0 and rolling_avg_values[start_idx] >= threshold:\n","                    start_idx -= 1\n","\n","                # Find the right boundary where the signal drops below the threshold\n","                while end_idx < len(rolling_avg_values) - 1 and rolling_avg_values[end_idx] >= threshold:\n","                    end_idx += 1\n","\n","                # Extract local_x and local_y based on these boundaries\n","                local_x = position_t_values[start_idx:end_idx]\n","                local_y = rolling_avg_values[start_idx:end_idx]\n","\n","                FWHM, left, right = find_FWTM(\n","                    local_x, local_y, 0.4)  # Here, the threshold is 0.5, i.e., half maximum\n","                fwhm_list.append(FWHM)\n","\n","                plt.scatter(position_t_values[peak],\n","                            rolling_avg_values[peak], color='red')  # Mark the peak\n","                plt.axvline(x=left, color='green',\n","                            linestyle='--')  # Mark the left half-maximum\n","                plt.axvline(x=right, color='green',\n","                            linestyle='--')  # Mark the right half-maximum\n","                plt.title(f'Track {track_id}')\n","                plt.xlabel('POSITION_T')\n","                plt.ylabel('Rolling Avg Intensity')\n","                plt.legend()\n","                pdf_pages.savefig(fig)\n","\n","            # Just collect the data in dictionaries\n","            if fwhm_list:  # if there are FWHM values\n","                fwhm_dict[track_id] = {\n","                    'fwhm_list': fwhm_list,\n","                    'Well': well,\n","                    'FOV': fov,\n","                    'Repeat': repeat,\n","                    'Condition': merged_spots_df[merged_spots_df['Well'] == well]['Condition'].iloc[0]\n","                }\n","\n","            if len(peaks) >= 2:\n","                peak_distances = np.diff(position_t_values[peaks])\n","                peak_distances_dict[track_id] = {\n","                    'distances': peak_distances,\n","                    'Well': well,\n","                    'FOV': fov,\n","                    'Repeat': repeat,\n","                    'Condition': merged_spots_df[merged_spots_df['Well'] == well]['Condition'].iloc[0]\n","                }\n","\n","# Add 'Condition' column to peak_info_df\n","peak_info_df = pd.merge(\n","peak_info_df, merged_spots_df[['Unique_ID', 'Condition']], on='Unique_ID', how='left') #This is now redundant\n","\n","# After all loops are done, create the final DataFrames once\n","final_results_fwhm = pd.DataFrame([\n","    {\n","        'Unique_ID': track_id,\n","        'Average_fwhm': np.mean(data['fwhm_list']),\n","        'Well': data['Well'],\n","        'FOV': data['FOV'],\n","        'Repeat': data['Repeat'],\n","        'Condition': data['Condition']\n","    }\n","    for track_id, data in fwhm_dict.items()\n","])\n","\n","final_results_average_peak_distances = pd.DataFrame([\n","    {\n","        'Unique_ID': track_id,\n","        'Average_Peak_Distance': np.mean(data['distances']),\n","        'Well': data['Well'],\n","        'FOV': data['FOV'],\n","        'Repeat': data['Repeat'],\n","        'Condition': data['Condition']\n","    }\n","    for track_id, data in peak_distances_dict.items()\n","])\n","\n","final_results_fwhm.to_csv(f\"{Results_Folder}/fwhm_info_all.csv\", index=False)\n","peak_info_df.to_csv(f\"{Results_Folder}/peak_info_all.csv\", index=False)\n","\n","# Create a DataFrame from the peak counts dictionary\n","peak_counts_df = pd.DataFrame.from_dict(peak_counts_dict, orient='index', columns=['Num_Peaks'])\n","peak_counts_df.index.name = 'Unique_ID'\n","\n","# Check if 'Num_Peaks' column already exists in merged_spots_df\n","if 'Num_Peaks' in merged_spots_df.columns:\n","    # If it exists, drop the existing 'Num_Peaks' column before merging\n","    merged_spots_df = merged_spots_df.drop(columns=['Num_Peaks'])\n","\n","# Merge with peak_counts_df\n","merged_spots_df = pd.merge(merged_spots_df, peak_counts_df, on='Unique_ID', how='left')\n","\n","# Initialize the PDF\n","with PdfPages(f\"{Results_Folder}/Combined_Plots.pdf\") as pdf_pages:\n","\n","    # Create a single figure with 2 subplots for boxplots\n","    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n","\n","    # Prepare the metrics to plot\n","    metrics_to_plot = ['Average_fwhm', 'Average_Peak_Distance']\n","    data_frames_to_use = [final_results_fwhm, final_results_average_peak_distances]\n","\n","    # Loop over both axes and metrics for boxplots\n","    for ax, metric, df in zip(axes, metrics_to_plot, data_frames_to_use):\n","\n","        # Save this data to a CSV file\n","        df[['Unique_ID', 'Well', 'FOV', 'Condition','Repeat', metric]].to_csv( # Add Repeat here\n","            f\"{Results_Folder}/data_for_{metric}.csv\", index=False)\n","\n","        # Create the boxplot using 'Condition' for grouping\n","        sns.boxplot(x='Condition', y=metric,\n","                    data=df, ax=ax, color='lightgray')\n","\n","        # Set title and labels\n","        ax.set_title(f\"{metric}\")\n","        ax.set_xlabel('Condition')  # Changed to 'Condition'\n","        ax.set_ylabel(metric)\n","\n","    # Save the figure to a PDF page\n","    plt.tight_layout()\n","    pdf_pages.savefig(fig)\n","    plt.close(fig)  # Close this figure before plotting the next one\n","\n","    # Create a new figure for the barplot\n","    fig, ax = plt.subplots(figsize=(20, 5))\n","\n","    # Assuming peak_info_df is your DataFrame\n","    peak_info_df['Has_Peak'] = peak_info_df['Has_Peak'].astype(int)\n","    agg_df = peak_info_df.groupby('Condition')[\n","        'Has_Peak'].mean().reset_index()  # Group by 'Condition'\n","\n","    # Create the bar plot using 'Condition' for grouping\n","    sns.barplot(x='Condition', y='Has_Peak',\n","                data=agg_df, ax=ax, color='lightblue')\n","\n","    # Set title and labels\n","    ax.set_title('Proportion of Tracks with Peaks by Condition')  # Changed to 'Condition'\n","    ax.set_xlabel('Condition')  # Changed to 'Condition'\n","    ax.set_ylabel('Proportion of Tracks with Peaks')\n","\n","    # Save the boxplot figure\n","    plt.tight_layout()\n","    pdf_pages.savefig(fig)\n","    plt.close(fig)\n","\n","# Save the updated merged_spots_df\n","merged_spots_df.to_csv(Results_Folder + '/' + 'merged_Spots_with_NumPeaks.csv', index=False)"],"metadata":{"id":"JxsytxwrRuZ1","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title #Plot proportion of Tracks with Peaks by Well (QC)\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","# Assuming peak_info_df is your DataFrame with columns 'Well', 'FOV', and 'Has_Peak'\n","\n","# Convert boolean to int for easier plotting and calculation\n","peak_info_df['Has_Peak'] = peak_info_df['Has_Peak'].astype(int)\n","\n","# Calculate the mean of Has_Peak per Well, which gives the proportion of True values\n","agg_df = peak_info_df.groupby('Well')['Has_Peak'].mean().reset_index()\n","\n","# Create a new figure\n","plt.figure(figsize=(20, 5))\n","\n","# Sort the 'Group' column alphabetically\n","group_order = merged_spots_df['Well'].sort_values().unique()\n","\n","# Create the bar plot for showing the proportion of True values in each Well\n","sns.barplot(x='Well', y='Has_Peak', data=agg_df, color='lightblue', order=group_order)\n","\n","# Set title and labels\n","plt.title('Proportion of Tracks with Peaks by Well')\n","plt.xlabel('Well')\n","plt.ylabel('Proportion of Tracks with Peaks')\n","plt.savefig(os.path.join(Results_Folder, 'QC/Proportion of Tracks with Peaks per condition.pdf'), bbox_inches='tight') # This line saves the heatmap\n","\n","# Show the plot\n","plt.show()"],"metadata":{"cellView":"form","id":"HOiHO6XfKSwM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","\n","# **Move to plotting notebook from here**\n","\n","\n","\n","---"],"metadata":{"id":"oFIioQQk0sXz"}}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}